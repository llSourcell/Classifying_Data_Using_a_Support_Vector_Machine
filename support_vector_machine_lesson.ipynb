{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Math of Intelligence (Week 1) - Support Vector Machine Classification\n",
    "\n",
    "\n",
    "## What will we do?\n",
    "\n",
    "We will build a Support Vector Machine that will find the optimal hyperplane that maximizes the margin between two toy data classes using gradient descent.  \n",
    "\n",
    "![alt text](http://opticalengineering.spiedigitallibrary.org/data/journals/optice/24850/oe_52_2_027003_f005.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "## What are some use cases for SVMs?\n",
    "\n",
    "-Classification, regression (time series prediction, etc) , outlier detection, clustering\n",
    "\n",
    "## Other Examples\n",
    "\n",
    "- Learning to use Scikit-learn's SVM function to classify images https://github.com/ksopyla/svm_mnist_digit_classification\n",
    "- Pulse classification, more useful dataset \n",
    "https://github.com/akasantony/pulse-classification-svm\n",
    "\n",
    "## How does an SVM compare to other ML algorithms?\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/mscpresentation-140722065852-phpapp01/95/msc-presentation-bioinformatics-7-638.jpg?cb=1406012610 \"Logo Title Text 1\")\n",
    "\n",
    "- As a rule of thumb, SVMs are great for relatively small data sets with fewer outliers. \n",
    "- Other algorithms (Random forests, deep neural networks, etc.) require more data but almost always come up with very robust models.\n",
    "- The decision of which classifier to use depends on your dataset and the general complexity of the problem.\n",
    "- \"Premature optimization is the root of all evil (or at least most of it) in programming.\" - Donald Knuth, CS Professor (Turing award speech 1974)  \n",
    "\n",
    "\n",
    "## What is a Support Vector Machine?\n",
    "\n",
    "It's a supervised machine learning algorithm which can be used for both classification or regression problems. But it's usually used for classification. Given 2 or more labeled classes of data, it acts as a discriminative classifier, formally defined by an optimal hyperplane that seperates all the classes. New examples that are then mapped into that same space can then be categorized based on on which side of the gap they fall.\n",
    "\n",
    "## What are Support Vectors?\n",
    "\n",
    "![alt text](https://www.dtreg.com/uploaded/pageimg/SvmMargin2.jpg \"Logo Title Text 1\")\n",
    " \n",
    "Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set, they are what help us build our SVM. \n",
    "\n",
    "## Whats a hyperplane?\n",
    "\n",
    "![alt text](http://slideplayer.com/slide/1579281/5/images/32/Hyperplanes+as+decision+surfaces.jpg \"Logo Title Text 1\")\n",
    "\n",
    "Geometry tells us that a hyperplane is a subspace of one dimension less than its ambient space. For instance, a hyperplane of an n-dimensional space is a flat subset with dimension n âˆ’ 1. By its nature, it separates the space into two half spaces.\n",
    "\n",
    "## Linear vs nonlinear classification?\n",
    "\n",
    "Sometimes our data is linearly seperable. That means for N classes with M features, we can learn a mapping that is a linear combination. (like y = mx + b). Or even a multidimensional hyperplane (y = x + z + b + q). No matter how many dimensions/features a set of classes have, we can represent the mapping using a linear function.\n",
    "\n",
    "But sometimes its not. Like if there was a quadratic mapping. Luckily for us SVMs can can efficiently perform a non-linear classification using what is called the kernel trick. I'll talk about that lter on in the course. \n",
    "\n",
    "![alt text](https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAuSAAAAJDlhYzcwMzhlLTA0MjYtNDEyYS1hMWM4LTE3Zjk5NDlhNzVkMQ.png \"Logo Title Text 1\")\n",
    "\n",
    "Alright let's get to building!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1057c0080>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nt4hIiFFxi4iEmP8HX67tlYF/BHAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103e12978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To help us perform math operations\n",
    "import numpy as np\n",
    "#to plot our data and model visually\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Step 1 - Define our data\n",
    "\n",
    "#Input data - Of the form [X value, Y value, Bias term]\n",
    "X = np.array([\n",
    "    [-2,4,-1],\n",
    "    [4,1,-1],\n",
    "    [1, 6, -1],\n",
    "    [2, 4, -1],\n",
    "    [6, 2, -1],\n",
    "])\n",
    "\n",
    "#Associated output labels - First 2 examples are labeled '-1' and last 3 are labeled '+1'\n",
    "y = np.array([-1,-1,1,1,1])\n",
    "\n",
    "#lets plot these examples on a 2D graph!\n",
    "#for each example\n",
    "for d, sample in enumerate(X):\n",
    "    # Plot the negative samples (the first 2)\n",
    "    if d < 2:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='_', linewidths=2)\n",
    "    # Plot the positive samples (the last 3)\n",
    "    else:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='+', linewidths=2)\n",
    "\n",
    "# Print a possible hyperplane, that is seperating the two classes.\n",
    "#we'll two points and draw the line between them (naive guess)\n",
    "plt.plot([-2,6],[6,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define our loss function (what to minimize) and our objective function (what to optimize)\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "We'll use the Hinge loss. This is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs).\n",
    "\n",
    "![alt text](http://i.imgur.com/OzCwzyN.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "c is the loss function, x the sample, y is the true label, f(x) the predicted label.\n",
    "\n",
    "![alt text](http://i.imgur.com/FZ7JcG3.png \"Logo Title Text 1\")\n",
    "\n",
    " \n",
    "#### Objective Function\n",
    "\n",
    "![alt text](http://i.imgur.com/I5NNu44.png \"Logo Title Text 1\")\n",
    "\n",
    "As you can see, our objective of a SVM consists of two terms. The first term is a regularizer, the heart of the SVM, the second term the loss. The regularizer balances between margin maximization and loss. We want to find the decision surface that is maximally far away from any data points.\n",
    "\n",
    "How do we minimize our loss/optimize for our objective (i.e learn)?\n",
    "\n",
    "We have to derive our objective function to get the gradients! Gradient descent ftw.  As we have two terms, we will derive them seperately using the sum rule in differentiation.\n",
    "\n",
    "\n",
    "![alt text](http://i.imgur.com/6uK3BnH.png \"Logo Title Text 1\")\n",
    "\n",
    "This means, if we have a misclassified sample, we update the weight vector w using the gradients of both terms, else if classified correctly,we just update w by the gradient of the regularizer.\n",
    "\n",
    "\n",
    "\n",
    "Misclassification condition \n",
    "\n",
    "![alt text](http://i.imgur.com/g9QLAyn.png \"Logo Title Text 1\")\n",
    "\n",
    "Update rule for our weights (misclassified)\n",
    "\n",
    "![alt text](http://i.imgur.com/rkdPpTZ.png \"Logo Title Text 1\")\n",
    "\n",
    "including the learning rate Î· and the regularizer Î»\n",
    "The learning rate is the length of the steps the algorithm makes down the gradient on the error curve.\n",
    "- Learning rate too high? The algorithm might overshoot the optimal point.\n",
    "- Learning rate too low? Could take too long to converge. Or never converge.\n",
    "\n",
    "The regularizer controls the trade off between the achieving a low training error and a low testing error that is the ability to generalize your classifier to unseen data. As a regulizing parameter we choose 1/epochs, so this parameter will decrease, as the number of epochs increases.\n",
    "- Regularizer too high? overfit (large testing error) \n",
    "- Regularizer too low? underfit (large training error) \n",
    "\n",
    "Update rule for our weights (correctly classified)\n",
    "\n",
    "![alt text](http://i.imgur.com/xTKbvZ6.png \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets perform stochastic gradient descent to learn the seperating hyperplane between both classes\n",
    "\n",
    "def svm_sgd_plot(X, Y):\n",
    "    #Initialize our SVMs weight vector with zeros (3 values)\n",
    "    w = np.zeros(len(X[0]))\n",
    "    #The learning rate\n",
    "    eta = 1\n",
    "    #how many iterations to train for\n",
    "    epochs = 100000\n",
    "    #store misclassifications so we can plot how they change over time\n",
    "    errors = []\n",
    "\n",
    "    #training part, gradient descent part\n",
    "    for epoch in range(1,epochs):\n",
    "        error = 0\n",
    "        for i, x in enumerate(X):\n",
    "            #misclassification\n",
    "            if (Y[i]*np.dot(X[i], w)) < 1:\n",
    "                #misclassified update for ours weights\n",
    "                w = w + eta * ( (X[i] * Y[i]) + (-2  *(1/epoch)* w) )\n",
    "                error = 1\n",
    "            else:\n",
    "                #correct classification, update our weights\n",
    "                w = w + eta * (-2  *(1/epoch)* w)\n",
    "        errors.append(error)\n",
    "        \n",
    "\n",
    "    #lets plot the rate of classification errors during training for our SVM\n",
    "    plt.plot(errors, '|')\n",
    "    plt.ylim(0.5,1.5)\n",
    "    plt.axes().set_yticklabels([])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Misclassified')\n",
    "    plt.show()\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = svm_sgd_plot(X,y)\n",
    "for d, sample in enumerate(X):\n",
    "    # Plot the negative samples\n",
    "    if d < 2:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='_', linewidths=2)\n",
    "    # Plot the positive samples\n",
    "    else:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='+', linewidths=2)\n",
    "\n",
    "# Add our test samples\n",
    "plt.scatter(2,2, s=120, marker='_', linewidths=2, color='yellow')\n",
    "plt.scatter(4,3, s=120, marker='+', linewidths=2, color='blue')\n",
    "\n",
    "# Print the hyperplane calculated by svm_sgd()\n",
    "x2=[w[0],w[1],-w[1],w[0]]\n",
    "x3=[w[0],w[1],w[1],-w[0]]\n",
    "\n",
    "x2x3 =np.array([x2,x3])\n",
    "X,Y,U,V = zip(*x2x3)\n",
    "ax = plt.gca()\n",
    "ax.quiver(X,Y,U,V,scale=1, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    
    "#they decrease over time! Our SVM is learning the optimal hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
